{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hrai/UTS_ML2019_ID12749723/blob/master/A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g89otovIo2CW",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 1 (12749723)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvreIrgyoyKf",
        "colab_type": "text"
      },
      "source": [
        "## Title - Support-Vector Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6kEP8T_Kat9",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY74IjFmfOMu",
        "colab_type": "text"
      },
      "source": [
        "In Machine Learning, there are a number of algorithms that help with predictions using patterns. Cortes & Vapnik (1995) proposed an improved classification learning machine called Support-Vector Network (SVN) in the paper titled ‘Support-Vector Networks’. This algorithm is used for both linear and non-linear classification purposes where dataset can be split into two groups. The authors have tested the accuracy and efficiency of the learning algorithm. Additionally, they’ve compared the results of performance tests against other learning algorithms like Decision Tree, Neural Network etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rG-uIo7eKfYt",
        "colab_type": "text"
      },
      "source": [
        "## Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLepfb86fP_c",
        "colab_type": "text"
      },
      "source": [
        "The paper acknowledges that the first algorithm for pattern recognition was published by R.A. Fisher (Fisher 1936). Later, Rosenblatt (1961) proposed perceptron or neural networks. It was followed by the discovery of back propagation algorithm (Cortes & Vapnik 1995). Building on the ideas from these scientists, the authors proposed a new machine learning algorithm called SVN. In this algorithm, the data that needs to be classified, also called input vectors, are mapped into a high-dimensional space. This non-linear feature space consists of an infinite number of planes. To model SVN classifier, a linear hyperplane needs to be constructed with the optimal margin. This hyperplane is called optimal hyperplane.\n",
        "\n",
        "Initially, there were 2 issues with the SVN - one conceptual and the other technical. The conceptual problem was the infinite dimensionality of the feature space. So not all the hyperplanes will be able to generalise the input vectors or training data well. The technical problem was in treating the high-dimensional spaces using the computer's power.\n",
        "\n",
        "In 1965, the conceptual part of this problem was solved by the introduction of optimal hyperplanes (Vapnik 1979). To draw optimal hyperplanes, support vectors are needed which form a small subset of the input dataset. The model created using this approach has high classification/generalisation ability even in an infinitely big dimensional space. In 1992, it was proven that instead of transforming input vectors and comparing with support vectors in feature space, the vectors can be compared in input space and then transform the result non-linearly into the feature space (Boser, Guyon & Vapnik 1992). Solving these two issues finally gave birth to SVN.\n",
        "\n",
        "SVN is a superior classification algorithm that can categorise data with high efficiency and minimal errors. The other virtue of SVN is its ability to scale seamlessly across multiple polynomial inputs which translates to hyperplanes. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fXzzyV5Klu6",
        "colab_type": "text"
      },
      "source": [
        "## Innovation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amC59E8PfVfE",
        "colab_type": "text"
      },
      "source": [
        "At the time of publication of this paper, SVN was a new learning technique. There were solutions similar to it but they were limited to datasets which were linearly separable. Data is not always spread out in the feature space in a clean homogenous fashion. This article proposes an innovative solution with high generalisation capability that uses polynomial input transformation. The proposed solution was tested against other classical learning algorithms. The benchmark study revealed that in fact, this solution was viable and showed signs of being efficient for machine learning purposes.\n",
        "\n",
        "<img src=\"https://github.com/hrai/UTS_ML2019_ID12749723/blob/master/optimal-hyperplane.PNG?raw=true\" width=\"500\"/>\n",
        "\n",
        "\n",
        "The soft margin classifier sits at the core of the non-linear SVN classifier. In order to create a hyperplane in feature space, the input vector needs to be transformed into a feature vector. Also, the concept of creating SVN derives from the dot products in Hilbert space (anderson and bahadur, 1966).\n",
        "\n",
        "The authors argue that the SVNs are efficient. The optimal decision functions are unique and they can be solved by either convex programming problem in the l-dimensional space of quadratic programming problem in the dual l+1 space of the parameters (Cortes & Vapnik 1995). Further, changing the function for the dot-product’s convolution produces different SVNs. This leads to the conclusion that it is universal whose decision function can be changed by only changing the dot-product. Finally, SVN has a feature that prevents the overfitting of data by executing Occan-Razor principle (Cortes & Vapnik 1995).\n",
        "\n",
        "In the article, SVN has been run through two sets of experiments. The first experiment involved creating patterns and testing the accuracy using 2nd degree polynomial decision surfaces or hyperplanes. The other involved real-life digit recognition problem. The results from the experiments were very compelling. The error probability didn’t exceed 3%. The classifier construction time was dependent on the number of support vectors rather than the degree of polynomial which is directly proportional to the dimensionality of the feature space. Additionally, the SVN with degree 2 or more had higher performance than the neural network created specifically for the purpose.\n",
        "\n",
        "In one of the experiments, image classification process was outlined. This opens up a number of avenues in it’s usages. The algorithm is versatile. The problem domain where it can be effective can range is numerous. In today’s machine learning focused market, many organisations can use this algorithm to build recommendation systems. Companies like Amazon, Netflix etc. have built an empire out of the intelligence provided by algorithms like SVN. Hence, this article proposed an idea which is applicable decades after it was proposed and we haven't fully realised it’s potential.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUjIycLkKpH7",
        "colab_type": "text"
      },
      "source": [
        "## Technical quality\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqtvGlaNfj3j",
        "colab_type": "text"
      },
      "source": [
        "The algorithm that was proposed in this paper was well tested. They had two sets of experiments to test their hypothesis. The first one was to test using 2 degree polynomial decision surface. It was followed by a digit recognition experiment.\n",
        "\n",
        "The first experiment is succinct. Additional information could have been provided as to how it was conducted. Some data samples would have made it clearer. On the useful side, they have different pictures of the input vectors clearly plotted in the feature space.\n",
        "\n",
        "<img src=\"https://github.com/hrai/UTS_ML2019_ID12749723/blob/master/distribution.PNG?raw=true\" width=\"500\"/>\n",
        "\n",
        "\n",
        "The process for second experiment that involved digit recognition was laid out in detail. They mentioned that the data samples were gathered from 2 different sources, US Postal Service being the source of images with smaller dataset. They explained that polynomials of numerous degrees were used to test the larger dataset whereas they used 4th degree polynomial classifier for the larger of the 2 datasets. They have used classifier benchmark information from other publications which helped them focus on the tests related to SVN. From the experiment, they deduced that the algorithm’s performance is not related to the dimensionality of the feature space. Consequently, SVNs don’t have the problem of overfitting.\n",
        "\n",
        "The NIST dataset was tested using 10 classifiers with an average training error of 0.02%. The combined error rate sat at 1.1% which performed way better than high performance classifiers like Neural Network that was constructed specifically for the purpose of image recognition. The figure 9 distinctly illustrates the error/accuracy rate of SVN against other classifiers.\n",
        "\n",
        "There is a comment about improving the performance of the classifier by modifying the dot-product but there’s not been an indication on how to do this or whether this will be covered in the next article. In addition, the conclusion mentions soft margins. A specific section to explain that would have made it easier to comprehend.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBsAJeUZKsD5",
        "colab_type": "text"
      },
      "source": [
        "## Application and X-factor\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Guo9TAhZfof9",
        "colab_type": "text"
      },
      "source": [
        "SVN is a specific tool that can be used for the purpose of classification of data sets. The research is innovative as it expands on pre-existing ideas which are limited to the data that can be linearly classified. The authors of the paper have proposed a general-purpose, highly efficient solution that scales very well. Evidently, from the experiments, it is safe to conclude that the learning system is more performant than purpose-built complex systems like neural networks. SVNs can definitely be used to create models that are general enough to be applied to a wide variety of problems yet keeping them light and efficient. This could be a precursor to many other innovative advancements in machine learning and not just in supervised learning problem space.\n",
        "\n",
        "SVN has been used in a number of real world use cases like image classification, face detection, bioinformatics etc. The algorithm can possibly be used to match protein sequence of cells and provide accurate prognosis so treatment can start early, reducing the number of fatalities.\n",
        "\n",
        "The most fascinating thing about this algorithm is its scalability and accuracy which trumps refined learning techniques. The concept is simple and easy to understand but the resulting algorithm is extremely powerful. This makes me realise that simple ideas can generate elegant solutions. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZwzobW_KvXi",
        "colab_type": "text"
      },
      "source": [
        "## Presentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0niWBTRpfsgc",
        "colab_type": "text"
      },
      "source": [
        "There was a lot of information in this paper. It would have been hard to follow without the help of the diagrams. All the mathematical formulas were clearly included and indices were explained in detail. \n",
        "\n",
        "On the other hand, there was some room for improvement. The print was not as crisp. This made it hard to read at times without zooming in. Definition of keywords like vectors, convolution, dot product, feature space, hyperplane, soft margin etc. would have made it easier for people without machine learning background easier to follow through with the paper. I also feel that the sections could be labelled with better headings. The flow of the content could be re-organised so that the information can be conveyed coherently.\n",
        "\n",
        "The paper was a seminal work in the field of machine learning. I believe the paper was targeted for machine learning academics as it is very technical. A slightly more detailed explanation of the theory would have made the content easier to read and understand.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii5pPp-QKxtc",
        "colab_type": "text"
      },
      "source": [
        "## References\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI4Xk7VMf6Te",
        "colab_type": "text"
      },
      "source": [
        "Boser, B.E., Guyon, I.M. & Vapnik, V.N. 1992, 'A training algorithm for optimal margin classifiers', paper presented to the *Proceedings of the fifth annual workshop on Computational learning theory*, Pittsburgh, Pennsylvania, USA.\n",
        "\n",
        "Cortes, C. & Vapnik, V. 1995, 'Support-Vector Networks', *Machine Learning*, vol. 20, no. 3, pp. 273-97.\n",
        "\n",
        "Fisher, R.A. 1936, 'THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS', *Annals of Eugenics*, vol. 7, no. 2, pp. 179-88.\n",
        "\n",
        "Rosenblatt, F. 1961, 'PRINCIPLES OF NEURODYNAMICS. PERCEPTRONS AND THE THEORY OF BRAIN MECHANISMS'.\n",
        "\n",
        "Vapnik, V.N. 1979, *Estimation of Dependences Based on Empirical Data in Russian*, Nauka, USSR.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akZXf_FmlxST",
        "colab_type": "text"
      },
      "source": [
        "##Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQiwK6OWl1ID",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/hrai/UTS_ML2019_ID12749723/blob/master/A1.ipynb"
      ]
    }
  ]
}